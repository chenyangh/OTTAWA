{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenyang/anaconda3/envs/pytorch2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def evaluate(predicted_string, alignment_string):\n",
    "    # # Example usage\n",
    "    # alignment_string = \"5-6 3-4 25p22 5-7 2p3 25p21 4-5 11-14 12-13 13-15 27p20 9-11 25p23 8-8 22-25 20-27 17-18 1-1 28-28 26p20 18-19 6p9 1-2 7-10 10-12 21-24 15-17 14-16 23-26\"\n",
    "    # predicted_string = \"5-6 3-4 25-22 ...\"  # Replace with your model's predicted alignments\n",
    "\n",
    "    def parse_alignments(alignment_string):\n",
    "        sure_alignments = set()\n",
    "        possible_alignments = set()\n",
    "\n",
    "        # Split the string into individual alignments\n",
    "        alignments = alignment_string.split()\n",
    "\n",
    "        for alignment in alignments:\n",
    "            if 'p' in alignment:\n",
    "                # Possible alignment\n",
    "                aligned_words = tuple(map(int, alignment.split('p')))\n",
    "                possible_alignments.add(aligned_words)\n",
    "            else:\n",
    "                # Sure alignment\n",
    "                aligned_words = tuple(map(int, alignment.split('-')))\n",
    "                sure_alignments.add(aligned_words)\n",
    "\n",
    "        return sure_alignments, possible_alignments\n",
    "\n",
    "    def calculate_f1(predicted_alignments, sure_alignments, possible_alignments):\n",
    "        a_and_s = len(predicted_alignments.intersection(sure_alignments))\n",
    "        a_and_p = len(predicted_alignments.intersection(possible_alignments))\n",
    "        prec = a_and_p / len(predicted_alignments) if len(predicted_alignments) > 0 else 0\n",
    "        rec = a_and_s / len(sure_alignments) if len(sure_alignments) > 0 else 0\n",
    "\n",
    "        if prec + rec == 0:\n",
    "            return 0\n",
    "        return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "    def calculate_aer(predicted_alignments, sure_alignments, possible_alignments):\n",
    "        a_and_s = len(predicted_alignments.intersection(sure_alignments))\n",
    "        a_and_p = len(predicted_alignments.intersection(possible_alignments))\n",
    "        return 1 - (a_and_s + a_and_p) / (len(predicted_alignments) + len(sure_alignments))\n",
    "\n",
    "    \n",
    "    # Parse alignments\n",
    "    sure_alignments, possible_alignments = parse_alignments(alignment_string)\n",
    "    predicted_alignments = parse_alignments(predicted_string)[0]  # Assuming model predicts only sure alignments\n",
    "\n",
    "    # Calculate metrics\n",
    "    f1_score = calculate_f1(predicted_alignments, sure_alignments, possible_alignments.union(sure_alignments))\n",
    "    aer = calculate_aer(predicted_alignments, sure_alignments, possible_alignments.union(sure_alignments))\n",
    "\n",
    "    # print(\"F1 Score:\", f1_score)\n",
    "    # print(\"AER:\", aer)\n",
    "    return {\"F1 Score\": f1_score, \"AER\": aer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming x and y are your data tensors\n",
    "# x: tensor of shape [m, dim]\n",
    "# y: tensor of shape [n, dim]\n",
    "def find_avg_vector2(x, y):\n",
    "    def compute_l2_distance(tensor_a, tensor_b):\n",
    "        return torch.norm(tensor_a - tensor_b, dim=1)\n",
    "\n",
    "    def objective_initialization(c, x, y, alpha):\n",
    "        avg_distance = (torch.mean(compute_l2_distance(x, c)) + torch.mean(compute_l2_distance(y, c))) / 2\n",
    "        distance_diff = torch.mean(torch.abs(compute_l2_distance(x, c) - compute_l2_distance(y, c)))\n",
    "        return alpha * avg_distance + (1 - alpha) * distance_diff\n",
    "\n",
    "    def variance_objective(c, x, y, lambda_weight):\n",
    "        x_distances = compute_l2_distance(x, c)\n",
    "        y_distances = compute_l2_distance(y, c)\n",
    "        var_x = torch.var(x_distances)\n",
    "        var_diff = torch.var(torch.abs(x_distances.view(-1, 1) - y_distances.view(1, -1)))\n",
    "        return lambda_weight * var_x + (1 - lambda_weight) * var_diff\n",
    "\n",
    "    # Parameters\n",
    "    alpha = 0.5\n",
    "    lambda_weight = 0.5\n",
    "    learning_rate = 0.01\n",
    "    max_iterations = 1000\n",
    "    convergence_threshold = 1e-6\n",
    "\n",
    "    # Initialize c\n",
    "    c = torch.mean(torch.cat([x.detach(), y.detach()], dim=0), dim=0).requires_grad_(True)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam([c], lr=learning_rate)\n",
    "\n",
    "    # Iterative Process\n",
    "    for iteration in range(max_iterations):\n",
    "        previous_loss = float('inf')\n",
    "\n",
    "        # Step 1: Initialization Loop\n",
    "        for _ in range(max_iterations):\n",
    "            optimizer.zero_grad()\n",
    "            loss_init = objective_initialization(c, x, y, alpha)\n",
    "            loss_init.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Early Stopping for Step 1\n",
    "            if torch.abs(previous_loss - loss_init) < convergence_threshold:\n",
    "                break\n",
    "            previous_loss = loss_init\n",
    "\n",
    "        # Step 2: Variance Minimization Loop\n",
    "        for _ in range(max_iterations):\n",
    "            optimizer.zero_grad()\n",
    "            loss_variance = variance_objective(c, x, y, lambda_weight)\n",
    "            loss_variance.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Early Stopping for Step 2\n",
    "            if torch.abs(previous_loss - loss_variance) < convergence_threshold:\n",
    "                break\n",
    "            previous_loss = loss_variance\n",
    "\n",
    "        # Convergence Check for Overall Process\n",
    "        if torch.abs(loss_variance - loss_init) < convergence_threshold:\n",
    "            print(f\"Converged in {iteration+1} overall iterations\")\n",
    "            break\n",
    "    \n",
    "    return c\n",
    "    # print(f\"Final vector c: {c.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/509 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/509 [00:04<34:52,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.761904761904762\n",
      "Average AER: 0.23809523809523814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/509 [00:39<29:51,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.6750436221444052\n",
      "Average AER: 0.32471118543710675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 21/509 [01:16<29:10,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7141522594270203\n",
      "Average AER: 0.2856560286437653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 31/509 [01:48<25:42,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7504492663572646\n",
      "Average AER: 0.24968703466244838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 41/509 [02:23<24:53,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7317034382412538\n",
      "Average AER: 0.26857455798104646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 51/509 [02:57<25:11,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7390502163334972\n",
      "Average AER: 0.2611827373143802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 61/509 [03:31<25:58,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7369637938901203\n",
      "Average AER: 0.2631198227997992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 71/509 [04:06<25:45,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7435583495800856\n",
      "Average AER: 0.2564440658623891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 81/509 [04:41<25:19,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7429960847244103\n",
      "Average AER: 0.25714448147121427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 91/509 [05:17<25:18,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7378728611816092\n",
      "Average AER: 0.2622056945734457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 101/509 [05:51<22:28,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7363340400357997\n",
      "Average AER: 0.26381108828542227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 111/509 [06:27<23:25,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7439736951296151\n",
      "Average AER: 0.256141943003476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 121/509 [07:02<23:07,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7438418751121871\n",
      "Average AER: 0.2561611285999849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 131/509 [07:38<22:08,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7493122647424545\n",
      "Average AER: 0.25069050967871215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 141/509 [08:11<20:40,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7501562535669754\n",
      "Average AER: 0.24993034903739614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 151/509 [08:41<17:03,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7498642347298775\n",
      "Average AER: 0.2502978673853552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 161/509 [09:14<20:33,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7530275271583695\n",
      "Average AER: 0.24708105762422525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 171/509 [09:47<15:34,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.75216406761542\n",
      "Average AER: 0.24791205931377772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 181/509 [10:22<19:19,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7534393252139334\n",
      "Average AER: 0.24666449807303922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 191/509 [10:58<19:40,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7537294933235837\n",
      "Average AER: 0.2463784438708151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 201/509 [11:33<18:01,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7556273980645699\n",
      "Average AER: 0.24449543653799358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 211/509 [12:07<17:21,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7497618346923692\n",
      "Average AER: 0.25035767528363495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 221/509 [12:40<15:22,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7504297007174776\n",
      "Average AER: 0.24965614777890216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 231/509 [13:15<15:47,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7526608300774748\n",
      "Average AER: 0.24745752871993298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 241/509 [13:49<14:53,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7501199928998645\n",
      "Average AER: 0.24995498024106066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 251/509 [14:21<14:42,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.751800945322013\n",
      "Average AER: 0.2482928709695129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 261/509 [14:53<13:01,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7541807152083706\n",
      "Average AER: 0.2459112936310295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 271/509 [15:29<13:49,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7541836124809074\n",
      "Average AER: 0.2459141170574789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 281/509 [16:05<13:47,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7550469253402042\n",
      "Average AER: 0.24505658567538702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 291/509 [16:41<12:40,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7559388534273581\n",
      "Average AER: 0.24416525078174747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 301/509 [17:17<11:50,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7571868637333801\n",
      "Average AER: 0.2429239951691582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 311/509 [17:53<11:50,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7569393438260651\n",
      "Average AER: 0.243167587360804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 321/509 [18:29<10:36,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7551131170800789\n",
      "Average AER: 0.24498517631384975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 331/509 [19:04<10:44,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7567663341949274\n",
      "Average AER: 0.24333695064975272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 341/509 [19:37<08:49,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7583466885471525\n",
      "Average AER: 0.24181709707829036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 351/509 [20:10<08:49,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7589984459931421\n",
      "Average AER: 0.24114206618298079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 361/509 [20:45<08:30,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7604929556035508\n",
      "Average AER: 0.23962252900488246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 371/509 [21:20<07:53,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7578807208915004\n",
      "Average AER: 0.2421738236986214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 381/509 [21:54<06:55,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7585008771101386\n",
      "Average AER: 0.24145982982822475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 391/509 [22:27<06:56,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7584854585490062\n",
      "Average AER: 0.24148713988586518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 401/509 [23:03<06:13,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7582720870160722\n",
      "Average AER: 0.24164577773657742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 411/509 [23:37<05:38,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7585320424735921\n",
      "Average AER: 0.24138314168428968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 421/509 [24:09<04:33,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7590293213959203\n",
      "Average AER: 0.2408855841405005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 431/509 [24:43<04:13,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7612607586339258\n",
      "Average AER: 0.23863711207910485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 441/509 [25:16<03:39,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7619355906418935\n",
      "Average AER: 0.23796049250674264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 451/509 [25:48<02:55,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.763408444592685\n",
      "Average AER: 0.23648616523147664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 461/509 [26:22<02:43,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.76359549539454\n",
      "Average AER: 0.23630291382843538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 471/509 [26:56<02:00,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7638096388398312\n",
      "Average AER: 0.2360842910483856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 481/509 [27:29<01:41,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7641100922743204\n",
      "Average AER: 0.23577991977694326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 491/509 [28:00<00:57,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7643437955973554\n",
      "Average AER: 0.23554612716032264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 501/509 [28:30<00:22,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7643329465183583\n",
      "Average AER: 0.23557312538163735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 508/509 [28:54<00:03,  3.41s/it]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 202\u001b[0m\n\u001b[1;32m    197\u001b[0m tgt_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/deen/en\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreadlines()[test_line_id]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# print(src_text)\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# print(tgt_text)\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m src_emb \u001b[38;5;241m=\u001b[39m \u001b[43mget_word_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m tgt_emb \u001b[38;5;241m=\u001b[39m get_word_embeddings(tgt_text, model, model\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[1;32m    206\u001b[0m src \u001b[38;5;241m=\u001b[39m src_text\n",
      "Cell \u001b[0;32mIn[4], line 56\u001b[0m, in \u001b[0;36mget_word_embeddings\u001b[0;34m(sentence, model, tokenizer)\u001b[0m\n\u001b[1;32m     53\u001b[0m     averaged_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mstack(subword_embeddings), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     54\u001b[0m     word_embeddings\u001b[38;5;241m.\u001b[39mappend(averaged_embedding)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word_embeddings) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentence\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m word_embeddings\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Analyze the hallucination labels\n",
    "import matplotlib.pyplot as plt\n",
    "from OTAlign.src.model_parts import *\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import ot\n",
    "from tqdm import tqdm\n",
    "\n",
    "def map_original_to_tokenized(x1, x2):\n",
    "    mapping = {}\n",
    "    tokenized_index = 0\n",
    "    original_index = 0\n",
    "\n",
    "    while original_index < len(x2) and tokenized_index < len(x1):\n",
    "        original_word = x2[original_index]\n",
    "        subword_sequence = ''\n",
    "\n",
    "        indices = []\n",
    "        while tokenized_index < len(x1) and (subword_sequence != original_word):\n",
    "            subword = x1[tokenized_index].lstrip('##')\n",
    "            if subword_sequence + subword == original_word[:len(subword_sequence + subword)]:\n",
    "                subword_sequence += subword\n",
    "                indices.append(tokenized_index)\n",
    "                tokenized_index += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not indices:\n",
    "            tokenized_index += 1\n",
    "        else:\n",
    "            mapping[original_index] = indices\n",
    "\n",
    "        original_index += 1\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def get_word_embeddings(sentence, model, tokenizer):\n",
    "    # Tokenize the sentence and get corresponding IDs\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens = inputs.tokens()[1:-1]\n",
    "    mapping = map_original_to_tokenized(tokens, sentence.split())\n",
    "\n",
    "    # Get BERT embeddings for each token\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(sentence, output_value='token_embeddings')[1:-1]\n",
    "\n",
    "    word_embeddings = []\n",
    "    for original_idx in range(len(sentence.split())):\n",
    "        subword_indices = mapping[original_idx]\n",
    "        subword_embeddings = [embeddings[idx] for idx in subword_indices]\n",
    "        averaged_embedding = torch.mean(torch.stack(subword_embeddings), dim=0)\n",
    "        word_embeddings.append(averaged_embedding)\n",
    "\n",
    "    assert len(word_embeddings) == len(sentence.split(\" \"))\n",
    "    return word_embeddings\n",
    "\n",
    "# word_level_emb = get_word_embeddings(\"This is an example sentence , which is meant to be encoded . Subwords are not a problem for this model .\", model, model.tokenizer)\n",
    "\n",
    "def find_avg_vector_cosine_similarity(vectors, vectors2=None):\n",
    "    def cosine_distance(x, vectors):\n",
    "        # Normalize the vectors\n",
    "        x_norm = x / x.norm(dim=1, keepdim=True)\n",
    "        vectors_norm = vectors / vectors.norm(dim=1, keepdim=True)\n",
    "        # Compute cosine similarity and convert to distance\n",
    "        cosine_sim = torch.mm(vectors_norm, x_norm.t())\n",
    "        return 1 - cosine_sim\n",
    "\n",
    "    def objective_function_uniform_cosine(x, vectors):\n",
    "        distances = cosine_distance(x, vectors).squeeze()\n",
    "        return torch.var(distances)\n",
    "\n",
    "    def objective_function_sum_cosine(x, vectors):\n",
    "        distances = cosine_distance(x, vectors).squeeze()\n",
    "        return distances.mean()\n",
    "\n",
    "    def objective_function_sum_uniform_cosine(x, vectors):\n",
    "        distances = cosine_distance(x, vectors).squeeze()\n",
    "        return distances.mean() + torch.var(distances)\n",
    "\n",
    "    combined_vectors = torch.cat([vectors, vectors2], dim=0)\n",
    "    x = torch.mean(combined_vectors.detach(), dim=0, keepdim=True).clone().detach().requires_grad_(True)\n",
    "\n",
    "    optimizer = torch.optim.Adam([x], lr=0.01)\n",
    "    for _ in range(3000):  # Number of iterations\n",
    "        optimizer.zero_grad()\n",
    "        loss = objective_function_sum_uniform_cosine(x, combined_vectors.detach())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    optimizer = torch.optim.Adam([x], lr=0.01)\n",
    "    for _ in range(3000):  # Number of iterations\n",
    "        optimizer.zero_grad()\n",
    "        loss = objective_function_uniform_cosine(x, combined_vectors.detach())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return x.detach().squeeze(0)\n",
    "\n",
    "def find_avg_vector_cuda(vectors, vectors2=None):\n",
    "    def objective_function_uniform_l2(x, vectors, ):\n",
    "        distances = torch.norm(vectors - x, dim=1)\n",
    "        return torch.var(distances)\n",
    "        \n",
    "    def objective_function_sum_l2(x, vectors, ):\n",
    "        distances = torch.norm(vectors - x, dim=1)\n",
    "        return distances.mean()\n",
    "    \n",
    "    def objective_function_sum_uniform_l2(x, vectors, ):\n",
    "        distances = torch.norm(vectors - x, dim=1)\n",
    "        return distances.mean() + torch.var(distances)\n",
    "    \n",
    "    \n",
    "    combined_vectors = torch.cat([vectors, vectors2], dim=0)\n",
    "    x = torch.mean(combined_vectors.detach(), dim=0, keepdim=True).clone().detach().requires_grad_(True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([x], lr=0.01)\n",
    "    for _ in range(3000):  # Number of iterations\n",
    "        optimizer.zero_grad()\n",
    "        loss = objective_function_sum_uniform_l2(x, combined_vectors.detach())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    optimizer = torch.optim.Adam([x], lr=0.01)\n",
    "    for _ in range(3000):  # Number of iterations\n",
    "        optimizer.zero_grad()\n",
    "        loss = objective_function_uniform_l2(x, combined_vectors.detach())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return x.detach().squeeze(0)\n",
    "\n",
    "def get_predicted_alignment(P):\n",
    "    # Apply argmax along each row\n",
    "    alignment_indices = np.argmax(P, axis=1)\n",
    "    null_idx = P.shape[1] - 1\n",
    "    # Generate alignment string\n",
    "    alignment_string = ' '.join(f'{j+1}-{i+1}' for i, j in enumerate(alignment_indices) if j != null_idx and sum(P[i]) > 0)\n",
    "    return alignment_string\n",
    "\n",
    "def rank_align_pairs(align):\n",
    "    align_pairs = align.split()\n",
    "    align_pairs = [pair.split('-') for pair in align_pairs if 'p' not in pair]\n",
    "    align_pairs = [(int(pair[0]), int(pair[1])) for pair in align_pairs]\n",
    "    align_pairs = sorted(align_pairs, key=lambda x: x[0])\n",
    "    return align_pairs \n",
    "\n",
    "m = 1\n",
    "epsilon = 0.01\n",
    "numItermax = 2000\n",
    "stopThr = 1e-6\n",
    "\n",
    "def get_ot_align(src_rep, mt_rep, m=m, epsilon=epsilon, numItermax=numItermax, stopThr=stopThr, w1=None, w2=None):\n",
    "    def convert_to_numpy(s1_weights, s2_weights, C):\n",
    "        if torch.is_tensor(s1_weights):\n",
    "            s1_weights = s1_weights.to('cpu').numpy()\n",
    "            s2_weights = s2_weights.to('cpu').numpy()\n",
    "        if torch.is_tensor(C):\n",
    "            C = C.to('cpu').numpy()\n",
    "        return s1_weights, s2_weights, C\n",
    "    \n",
    "    \n",
    "    C = compute_distance_matrix_l2(mt_rep, src_rep, 0.0)\n",
    "    # C = compute_distance_matrix_cosine(mt_rep, src_rep, 0.0)\n",
    "    \n",
    "    s1_weights, s2_weights = compute_weights_uniform(mt_rep, src_rep)\n",
    "    # s1_weights, s2_weights = compute_weights_norm(mt_rep, src_rep)\n",
    "    \n",
    "    s2_weights[-1] = 1.0\n",
    "    s1_weights = s1_weights / s1_weights.sum()\n",
    "    s2_weights = s2_weights / s2_weights.sum()\n",
    "    \n",
    "    \n",
    "    # s2_weights[:-1].sum() * 0.5\n",
    "    s1_weights, s2_weights, C = convert_to_numpy(s1_weights, s2_weights, C)\n",
    "\n",
    "    m = np.min((np.sum(s1_weights), np.sum(s2_weights))) * m\n",
    "    # P = ot.partial.entropic_partial_wasserstein(s1_weights, s2_weights, C, reg=epsilon, m=m, stopThr=stopThr, numItermax=numItermax)\n",
    "    P = ot.emd(s1_weights, s2_weights, C)\n",
    "    # P = min_max_scaling(P)\n",
    "    # P = ot.unbalanced.sinkhorn_stabilized_unbalanced(s1_weights, s2_weights, C, reg=epsilon, reg_m=(0.1, 1), stopThr=stopThr, numItermax=numItermax)\n",
    "    # P = min_max_scaling(P)\n",
    "    \n",
    "    # P = ot.emd(s1_weights / s1_weights.sum(), s2_weights / s2_weights.sum(), C)\n",
    "    tmp = 0\n",
    "    \n",
    "    return P, C\n",
    "\n",
    "num_samples = len(open('data/deen/alignmentDeEn.talp', 'r').readlines())\n",
    "\n",
    "results = []\n",
    "for test_line_id in tqdm(range(num_samples)):\n",
    "    # test_line_id = 5\n",
    "    gold_alignment = open('data/deen/alignmentDeEn.talp', 'r').readlines()[test_line_id]\n",
    "    src_text = open('data/deen/de', 'r', encoding='latin-1').readlines()[test_line_id].strip()\n",
    "    tgt_text = open('data/deen/en', 'r', encoding='latin-1').readlines()[test_line_id].strip()\n",
    "\n",
    "    # print(src_text)\n",
    "    # print(tgt_text)\n",
    "\n",
    "    src_emb = get_word_embeddings(src_text, model, model.tokenizer)\n",
    "    tgt_emb = get_word_embeddings(tgt_text, model, model.tokenizer)\n",
    "\n",
    "\n",
    "    src = src_text\n",
    "    mt = tgt_text\n",
    "\n",
    "    src_rep = torch.stack(src_emb, dim=0).cuda('cuda:1')\n",
    "    mt_rep = torch.stack(tgt_emb, dim=0).cuda('cuda:1')\n",
    "\n",
    "    avg_src_rep = find_avg_vector_cuda(src_rep, mt_rep)\n",
    "    # avg_src_rep = find_avg_vector_cosine_similarity(src_rep, mt_rep)\n",
    "    \n",
    "    avg_src_rep = avg_src_rep.to(mt_rep)\n",
    "    src_rep = torch.cat([src_rep, avg_src_rep.unsqueeze(0)], dim=0)\n",
    "\n",
    "    P, C = get_ot_align(src_rep, mt_rep)\n",
    "\n",
    "\n",
    "    null_idx = P.shape[1] - 1\n",
    "    # clone numpy array P\n",
    "    threshod = 1 / P.shape[1] * 0.5\n",
    "    P_copy = P.copy()\n",
    "    P_copy[P_copy<threshod] = 0\n",
    "\n",
    "    # P_copy[P_copy<0.0001] = null_idx\n",
    "    predicted_align = get_predicted_alignment(P_copy)\n",
    "    # print(rank_align_pairs(gold_alignment))\n",
    "    # print(rank_align_pairs(predicted_align))\n",
    "    f1_aer = evaluate(predicted_align, gold_alignment.strip())\n",
    "    results.append(f1_aer)\n",
    "    # print(f1_aer)\n",
    "    if test_line_id % 10 == 0:\n",
    "        print(\"Average F1 Score:\", np.mean([result['F1 Score'] for result in results]))\n",
    "        print(\"Average AER:\", np.mean([result['AER'] for result in results]))\n",
    "    \n",
    "\n",
    "    DEBUG = False\n",
    "    if DEBUG:\n",
    "        \n",
    "        x = src_text.split()\n",
    "        x.append('NULL')\n",
    "        y = tgt_text.split()\n",
    "\n",
    "        fig_width = max(16, len(x))  # Doubling the width to accommodate both heatmaps\n",
    "        fig_height = max(6, len(y) * 0.5)  # 0.5 inch per row\n",
    "        \n",
    "        plt.clf()\n",
    "        \n",
    "        plt.figure(figsize=(fig_width, fig_height))\n",
    "\n",
    "        # Heatmap for P\n",
    "        plt.subplot(1, 2, 1)  # 1 row, 2 columns, first subplot\n",
    "        plt.imshow(P, cmap='hot', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.xticks(ticks=np.arange(len(x)), labels=x, rotation=90)\n",
    "        plt.yticks(ticks=np.arange(len(y)), labels=y)\n",
    "        plt.title(\"Heatmap of P\")\n",
    "        plt.grid(which='major', axis='both', linestyle='-', color='white', linewidth=0.5)\n",
    "        plt.gca().set_xticks(np.arange(-0.5, len(x), 1), minor=True)\n",
    "        plt.gca().set_yticks(np.arange(-0.5, len(y), 1), minor=True)\n",
    "\n",
    "        # Heatmap for C\n",
    "        plt.subplot(1, 2, 2)  # 1 row, 2 columns, second subplot\n",
    "        plt.imshow(C, cmap='hot', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.xticks(ticks=np.arange(len(x)), labels=x, rotation=90)\n",
    "        plt.yticks(ticks=np.arange(len(y)), labels=y)\n",
    "        plt.title(\"Heatmap of C\")\n",
    "        plt.grid(which='major', axis='both', linestyle='-', color='white', linewidth=0.5)\n",
    "        plt.gca().set_xticks(np.arange(-0.5, len(x), 1), minor=True)\n",
    "        plt.gca().set_yticks(np.arange(-0.5, len(y), 1), minor=True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        tmp = 0\n",
    "\n",
    "# get average results\n",
    "print(\"Average F1 Score:\", np.mean([result['F1 Score'] for result in results]))\n",
    "print(\"Average AER:\", np.mean([result['AER'] for result in results]))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.7645508832945358\n",
      "Average AER: 0.23536543027460174\n"
     ]
    }
   ],
   "source": [
    "print(\"Average F1 Score:\", np.mean([result['F1 Score'] for result in results]))\n",
    "print(\"Average AER:\", np.mean([result['AER'] for result in results]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(13, 13), (14, 13), (15, 14), (16, 8), (17, 10), (17, 9), (17, 12), (17, 11), (18, 15)]\n",
      "[(3, 1), (4, 4), (5, 10), (6, 7), (7, 9), (8, 2), (9, 8), (10, 6), (11, 5), (12, 12), (13, 13), (15, 14), (17, 11), (18, 15)]\n",
      "F1 Score: 0.34782608695652173\n",
      "AER: 0.5652173913043479\n"
     ]
    }
   ],
   "source": [
    "def get_predicted_alignment(P):\n",
    "    # Apply argmax along each row\n",
    "    alignment_indices = np.argmax(P, axis=1)\n",
    "    null_idx = P.shape[1] - 1\n",
    "    # Generate alignment string\n",
    "    alignment_string = ' '.join(f'{j+1}-{i+1}' for i, j in enumerate(alignment_indices) if j != null_idx and sum(P[i]) > 0)\n",
    "    return alignment_string\n",
    "\n",
    "def rank_align_pairs(align):\n",
    "    align_pairs = align.split()\n",
    "    align_pairs = [pair.split('-') for pair in align_pairs if 'p' not in pair]\n",
    "    align_pairs = [(int(pair[0]), int(pair[1])) for pair in align_pairs]\n",
    "    align_pairs = sorted(align_pairs, key=lambda x: x[0])\n",
    "    return align_pairs\n",
    "\n",
    "null_idx = P.shape[1] - 1\n",
    "# clone numpy array P\n",
    "threshod = 1 / P.shape[1] * 0.5\n",
    "P_copy = P.copy()\n",
    "P_copy[P_copy<threshod] = 0\n",
    "\n",
    "# P_copy[P_copy<0.0001] = null_idx\n",
    "predicted_align = get_predicted_alignment(P_copy)\n",
    "print(rank_align_pairs(gold_alignment))\n",
    "print(rank_align_pairs(predicted_align))\n",
    "evaluate(predicted_align, gold_alignment.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02807018, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03859649])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_copy[2]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
