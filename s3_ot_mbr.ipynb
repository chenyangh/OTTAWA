{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 02:07:23.420828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Processing: 100%|██████████| 508/508 [08:32<00:00,  1.01s/it, F1=0.7463, AER=0.2540, Precision=0.7298, Recall=0.7637]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maj_vote_fwd: F1: 0.7475, AER: 0.2525, Precision: 0.7477, Recall: 0.7473\n",
      "maj_vote_rev: F1: 0.7463, AER: 0.2540, Precision: 0.7298, Recall: 0.7637\n",
      "union: F1: 0.7287, AER: 0.2720, Precision: 0.6894, Recall: 0.7728\n",
      "intersection: F1: 0.7288, AER: 0.2720, Precision: 0.6896, Recall: 0.7728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from my_ot_algorithm import *\n",
    "from other_ot_algorithms import *\n",
    "from align_utils import *\n",
    "\n",
    "src = 'de'\n",
    "tgt = 'en'\n",
    "\n",
    "num_samples = len(open(f'PMI-Align/data/{src}-{tgt}/text.{src}', 'r').readlines())\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "gold_aligen_list = []\n",
    "\n",
    "with tqdm(total=num_samples, desc='Processing') as pbar:\n",
    "# if True:\n",
    "#     test_line_id = 508\n",
    "    for test_line_id in range(num_samples):        \n",
    "        gold_alignment = open(f'PMI-Align/data/{src}-{tgt}/gold.{src}-{tgt}.aligned', 'r').readlines()[test_line_id]\n",
    "        src_text = open(f'PMI-Align/data/{src}-{tgt}/text.{src}', 'r').readlines()[test_line_id].strip()\n",
    "        tgt_text = open(f'PMI-Align/data/{src}-{tgt}/text.{tgt}', 'r',).readlines()[test_line_id].strip()\n",
    "        \n",
    "        if len(src_text.split()) == 0:\n",
    "            print('Finished @', test_line_id)\n",
    "            print('Processed', len(results), 'samples')\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            src_emb_ = get_word_embeddings(src_text, model, tokenizer)\n",
    "            tgt_emb_ = get_word_embeddings(tgt_text, model, tokenizer)\n",
    "        except Exception as e:\n",
    "            print('error emb @', test_line_id)\n",
    "            continue\n",
    "        \n",
    "        gold_aligen_list.append(gold_alignment)\n",
    "        \n",
    "        \"\"\" Config Format\n",
    "        config: {\n",
    "          'method': 'emd' | 'sink'| 'p_sink' | 'p_sink_fwd' | 'p_sink_rev' | 'unk_sink_fwd' | 'unk_sink_rev' | 'unk_p_sink_fwd' | 'unk_p_sink_rev',\n",
    "          'epsilon': 0.1,\n",
    "          'relax_ratio': 1.5,\n",
    "          'distance_metric': 'cosine' | 'l2' # default: cosine\n",
    "          }\n",
    "        \"\"\"\n",
    "\n",
    "        P_dict = {}\n",
    "        \n",
    "\n",
    "        # NOTE: emd is for hard full-to-full mapping\n",
    "        # config_emd = {'method': 'emd'}\n",
    "        # P_emd, C = get_ot_map(src_emb_, tgt_emb_, config_emd)\n",
    "        # P_dict['hard_f2f'] = P_emd\n",
    "        \n",
    "        # NOTE: sinkhorn is for soft full-to-full mapping\n",
    "        config_sink = {'method': 'sink', 'epsilon': 0.1}\n",
    "        P_sink, _ = get_ot_map(src_emb_, tgt_emb_, config_sink)\n",
    "        P_dict['f2f'] = P_sink\n",
    "        \n",
    "        # NOTE: p_sink is for soft partial-to-partial mapping\n",
    "        config_p_sink = {'method': 'p_sink', 'epsilon': 0.1, 'relax_ratio': 1.5}\n",
    "        P_p_sink, _ = get_ot_map(src_emb_, tgt_emb_, config_p_sink)\n",
    "        P_dict['p2p'] = P_p_sink\n",
    "        \n",
    "        # NOTE: p_sink_f2p is for full-to-partial mapping\n",
    "        config_p_sink_f2p = {'method': 'p_sink_f2p', 'epsilon': 0.1, 'relax_ratio': 1.5}\n",
    "        P_p_sink_f2p, _ = get_ot_map(src_emb_, tgt_emb_, config_p_sink_f2p)\n",
    "        P_dict['f2p'] = P_p_sink_f2p\n",
    "        \n",
    "        # NOTE: p_sink_p2f is for soft partial-to-full mapping\n",
    "        config_p_sink_p2f = {'method': 'p_sink_p2f', 'epsilon': 0.1, 'relax_ratio': 1.5}\n",
    "        P_p_sink_p2f, _ = get_ot_map(src_emb_, tgt_emb_, config_p_sink_p2f)\n",
    "        P_dict['p2f'] = P_p_sink_p2f\n",
    "        \n",
    "        # NOTE: p_sink_f2f_fwd is for foward align with full-to-full sinkhorn\n",
    "        config_p_sink_f2f_fwd = {'method': 'p_sink_f2f_fwd', 'epsilon': 0.1}\n",
    "        P_p_sink_f2f_fwd, _ = get_ot_map(src_emb_, tgt_emb_, config_p_sink_f2f_fwd)\n",
    "        P_dict['f2f_fwd'] = P_p_sink_f2f_fwd\n",
    "        \n",
    "        # NOTE: p_sink_f2f_rev is for reverse align with full-to-full sinkhorn\n",
    "        config_p_sink_f2f_rev = {'method': 'p_sink_f2f_rev', 'epsilon': 0.1}     \n",
    "        P_p_sink_f2f_rev, _ = get_ot_map(src_emb_, tgt_emb_, config_p_sink_f2f_rev)\n",
    "        P_dict['f2f_rev'] = P_p_sink_f2f_rev\n",
    "        \n",
    "        # # NOTE: p_sink_p2f_fwd is for forward align with partial-to-full sinkhorn\n",
    "        # config_p_sink_p2f_fwd = {'method': 'p_sink_p2f_fwd', 'epsilon': 0.1, 'relax_ratio': 1.5}\n",
    "        # P_p_sink_p2f_fwd, _ = get_ot_map(src_emb_, tgt_emb_, config_p_sink_p2f_fwd)\n",
    "        # P_dict['p2f_fwd'] = P_p_sink_p2f_fwd\n",
    "        \n",
    "        # # NOTE: p_sink_f2p_rev is for reverse align with full-to-partial sinkhorn\n",
    "        # config_p_sink_f2p_rev = {'method': 'p_sink_f2p_rev', 'epsilon': 0.1, 'relax_ratio': 1.5}\n",
    "        # P_p_sink_f2p_rev, _ = get_ot_map(src_emb_, tgt_emb_, config_p_sink_f2p_rev)\n",
    "        # P_dict['f2p_rev'] = P_p_sink_f2p_rev\n",
    "        \n",
    "        # # # ===== not recommended =====\n",
    "        # # NOTE: p_sink_f2p_fwd is for forward align with partial-to-full sinkhorn\n",
    "        # p_sink_f2p_fwd = {'method': 'p_sink_f2p_fwd', 'epsilon': 0.1, 'relax_ratio': 1.5}\n",
    "        # P_p_sink_f2p_fwd, _ = get_ot_map(src_emb_, tgt_emb_, p_sink_f2p_fwd)\n",
    "        # P_dict['f2p_fwd'] = P_p_sink_f2p_fwd\n",
    "        \n",
    "        # # ===== not recommended =====\n",
    "        # # NOTE: p_sink_p2f_rev is for reverse align with partial-to-full sinkhorn\n",
    "        # p_sink_p2f_rev = {'method': 'p_sink_p2f_rev', 'epsilon': 0.1, 'relax_ratio': 1.5}\n",
    "        # P_p_sink_p2f_rev, _ = get_ot_map(src_emb_, tgt_emb_, p_sink_p2f_rev)\n",
    "        # P_dict['p2f_rev'] = P_p_sink_p2f_rev\n",
    "        \n",
    "        \n",
    "        fwd_str, rev_str = ot_mbr(P_dict, C)\n",
    "        \n",
    "        if 'maj_vote_fwd' not in results:\n",
    "            results['maj_vote_fwd'] = [fwd_str]\n",
    "        else:\n",
    "            results['maj_vote_fwd'].append(fwd_str)\n",
    "        \n",
    "        if 'maj_vote_rev' not in results:\n",
    "            results['maj_vote_rev'] = [rev_str]\n",
    "        else:\n",
    "            results['maj_vote_rev'].append(rev_str)        \n",
    "        \n",
    "        if 'union' not in results:\n",
    "            results['union'] = [get_joint_alignments(fwd_str, rev_str, method='union')]\n",
    "        else:\n",
    "            results['union'].append(get_joint_alignments(fwd_str, rev_str,  method='union'))\n",
    "        \n",
    "        if 'intersection' not in results:\n",
    "            results['intersection'] = [get_joint_alignments(fwd_str, rev_str, method='intersection')]\n",
    "        else:\n",
    "            results['intersection'].append(get_joint_alignments(fwd_str, rev_str))\n",
    "            \n",
    "        \n",
    "        tmp = 0\n",
    "        # ====== evaluation ======\n",
    "        eval_result = evaluate_corpus_level(results['maj_vote_rev'], gold_aligen_list)\n",
    "        f1 = eval_result['F1 Score']\n",
    "        aer = eval_result['AER']\n",
    "        precision = eval_result['Precision']\n",
    "        recall = eval_result['Recall']\n",
    "        \n",
    "        avg_f1_str = f'{f1:.4f}'\n",
    "        avg_aer_str = f'{aer:.4f}'\n",
    "        avg_pre_str = f'{precision:.4f}'\n",
    "        avg_rec_str = f'{recall:.4f}'\n",
    "        \n",
    "        pbar.set_postfix({'F1': avg_f1_str, 'AER': avg_aer_str, 'Precision': avg_pre_str, 'Recall': avg_rec_str})\n",
    "        pbar.update(1)\n",
    "\n",
    "for merge_method, results_list in results.items():\n",
    "    eval_result = evaluate_corpus_level(results_list, gold_aligen_list)\n",
    "    f1 = eval_result['F1 Score']\n",
    "    aer = eval_result['AER']\n",
    "    precision = eval_result['Precision']\n",
    "    recall = eval_result['Recall']\n",
    "    print(f'{merge_method}: F1: {f1:.4f}, AER: {aer:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}') \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maj_vote_fwd: F1: 0.7475, AER: 0.2525, Precision: 0.7477, Recall: 0.7473\n",
      "maj_vote_rev: F1: 0.7463, AER: 0.2540, Precision: 0.7298, Recall: 0.7637\n"
     ]
    }
   ],
   "source": [
    "for merge_method, results_list in results.items():\n",
    "    eval_result = evaluate_corpus_level(results_list, gold_aligen_list)\n",
    "    f1 = eval_result['F1 Score']\n",
    "    aer = eval_result['AER']\n",
    "    precision = eval_result['Precision']\n",
    "    recall = eval_result['Recall']\n",
    "    print(f'{merge_method}: F1: {f1:.4f}, AER: {aer:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(13, 13), (14, 13), (15, 14), (16, 8), (17, 10), (17, 9), (17, 12), (17, 11), (18, 15)]\n",
      "[(3, 1), (4, 4), (5, 10), (6, 7), (7, 9), (8, 2), (9, 8), (10, 6), (11, 5), (12, 12), (13, 13), (15, 14), (17, 11), (18, 15)]\n",
      "F1 Score: 0.34782608695652173\n",
      "AER: 0.5652173913043479\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0], 1: [1], 2: [2], 3: [4], 4: [5], 5: [6], 6: [7], 7: [8, 9], 8: [10, 11, 12], 9: [13], 10: [15]}\n"
     ]
    }
   ],
   "source": [
    "def map_original_to_sentencepiece(x1, x2):\n",
    "    mapping = {}\n",
    "    tokenized_index = 0\n",
    "    original_index = 0\n",
    "\n",
    "    # Replace with the actual underscore character used in your SentencePiece tokenization\n",
    "    sentence_piece_underscore = '▁'\n",
    "\n",
    "    while original_index < len(x2) and tokenized_index < len(x1):\n",
    "        original_word = x2[original_index]\n",
    "        subword_sequence = ''\n",
    "\n",
    "        indices = []\n",
    "        while tokenized_index < len(x1) and (subword_sequence != original_word):\n",
    "            subword = x1[tokenized_index].lstrip(sentence_piece_underscore)\n",
    "            if subword and (subword_sequence + subword == original_word[:len(subword_sequence) + len(subword)]):\n",
    "                subword_sequence += subword\n",
    "                indices.append(tokenized_index)\n",
    "            tokenized_index += 1\n",
    "\n",
    "        # Special handling for tokens that are just the SentencePiece underscore or punctuation\n",
    "        if not indices and x1[tokenized_index - 1].strip(sentence_piece_underscore) == '':\n",
    "            indices.append(tokenized_index - 1)\n",
    "\n",
    "        if indices:\n",
    "            mapping[original_index] = indices\n",
    "\n",
    "        original_index += 1\n",
    "\n",
    "    return mapping\n",
    "\n",
    "# Example usage with SentencePiece tokenized and original sentences\n",
    "x1_sp = ['▁Wir', '▁glauben', '▁nicht', '▁', ',', '▁daß', '▁wir', '▁nur', '▁Ros', 'inen', '▁heraus', 'pi', 'cken', '▁sollten', '▁', '.']\n",
    "x2 = ['Wir', 'glauben', 'nicht', ',', 'daß', 'wir', 'nur', 'Rosinen', 'herauspicken', 'sollten', '.']\n",
    "mapping = map_original_to_sentencepiece(x1_sp, x2)\n",
    "print(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 2), (2, 1), (0, 0)}\n"
     ]
    }
   ],
   "source": [
    "def parse_alignment_string(alignment_str):\n",
    "    return set(tuple(map(int, pair.split('-'))) for pair in alignment_str.split())\n",
    "\n",
    "def grow_diag_final(forward_str, reverse_str, src_len, tgt_len):\n",
    "    forward = parse_alignment_string(forward_str)\n",
    "    reverse = parse_alignment_string(reverse_str)\n",
    "\n",
    "    # Intersection\n",
    "    alignment = forward & reverse\n",
    "\n",
    "    # Define neighbors (8 surrounding points)\n",
    "    neighbors = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "\n",
    "    # Grow step\n",
    "    added = True\n",
    "    while added:\n",
    "        added = False\n",
    "        new_points = set()\n",
    "        for s, t in alignment:\n",
    "            for dx, dy in neighbors:\n",
    "                ns, nt = s + dx, t + dy\n",
    "                if 0 <= ns < src_len and 0 <= nt < tgt_len and (ns, nt) not in alignment:\n",
    "                    if (ns, nt) in forward or (ns, nt) in reverse:\n",
    "                        new_points.add((ns, nt))\n",
    "                        added = True\n",
    "        alignment.update(new_points)\n",
    "\n",
    "    # Final step\n",
    "    for s in range(src_len):\n",
    "        for t in range(tgt_len):\n",
    "            if all((s, t_) not in alignment for t_ in range(tgt_len)) and (s, t) in forward:\n",
    "                alignment.add((s, t))\n",
    "            if all((s_, t) not in alignment for s_ in range(src_len)) and (s, t) in reverse:\n",
    "                alignment.add((s, t))\n",
    "\n",
    "    return alignment\n",
    "\n",
    "# Example usage\n",
    "forward_str = '0-0 1-2'\n",
    "reverse_str = '0-0 2-1'\n",
    "src_len = 3  # Length of source sentence\n",
    "tgt_len = 3  # Length of target sentence\n",
    "\n",
    "result = grow_diag_final(forward_str, reverse_str, src_len, tgt_len)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Grow-diag-final output: {(0, 0), (2, 2)}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
